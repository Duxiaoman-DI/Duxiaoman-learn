# Duxiaoman-learn

数据智能领域优秀文章分享（持续更新中...)

   ### NLP方向

 - cosFormer：Rethinking Softmax in Attention 
 -PDF: https://arxiv.org/abs/2202.08791
   
 - Transformer Quality in Linear Time  
 -PDF: https://arxiv.org/abs/2202.10447
   
 - Contextual Representation Learning beyond Masked Language Modeling 
 -PDF:    http://wiki.duxiaoman-int.com/download/attachments/197406888/2204.04163.pdf?version=1&modificationDate=1652870758000&api=v2
 - cosFormer：Rethinking Softmax in Attention 
 -PDF: https://arxiv.org/abs/2202.08791
  
 - Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection 
 -PDF: https://arxiv.org/pdf/2203.14380.pdf

 - Robust Lottery Tickets for Pre-trained Language Models 
 -PDF: https://aclanthology.org/2022.acl-long.157/
     
 - Compression of Generative Pre-trained Language Models via Quantization 
 -PDF: https://arxiv.org/pdf/2203.10705.pdf
      
 - Filter-enhanced MLP is All You Need for Sequential Recommendation	
 -PDF: https://arxiv.org/abs/2202.13556	
     
 - A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models	
 -PDF:    https://arxiv.org/pdf/2202.13392.pdf
      

 - Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents  
 -PDF:https://arxiv.org/abs/2203.02898
 
 -  Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data	 
 -PDF:    https://arxiv.org/abs/2203.08773
   
 -  BERT Learns to Teach: Knowledge Distillation with Meta Learning	
 -PDF: https://aclanthology.org/2022.acl-long.485/
   
 -  A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space	
 -PDF:    https://aclanthology.org/2022.acl-long.336.pdf
   
 -  LongT5: Efficient Text-To-Text Transformer for Long Sequences	
 -PDF: https://arxiv.org/abs/2112.07916
   
 -  Debiased Contrastive Learning of Unsupervised Sentence Representations	
 -PDF: https://arxiv.org/pdf/2205.00656.pdf
   
 -  DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings	
 -PDF: https://aclanthology.org/2022.naacl-main.311.pdf
   
 -  Poolingformer: Long Document Modeling with Pooling Attention	
 -PDF: https://arxiv.org/pdf/2105.04371.pdf
   
 -  Dict-BERT: Enhancing Language Model Pre-training with Dictionary	
 -PDF: https://arxiv.org/pdf/2110.06490.pdf
   
 -  FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization 
 -PDF: https://arxiv.org/abs/2205.07830
   
 -  Efficient Long-Text Understanding with Short-Text Models	
 -PDF: https://arxiv.org/pdf/2208.00748.pdf
   
 -  Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification	 
 -PDF:    https://aclanthology.org/2022.acl-long.158/
   
 -  ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding	 
 -PDF:    https://arxiv.org/abs/2109.04380
   
 -  CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding	 
 -PDF:    https://openreview.net/forum?id=Ozk9MrX1hvA
   
 -  An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification	 
 -PDF:    https://arxiv.org/abs/2210.05529
 
 
      
   ### 因果推断方向

 - Introduction to Causal Inference from a Machine Learning Perspective
 -PDF: https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf

- Quasi-Oracle Estimation of Heterogeneous Treatment Effects
-PDF: https://arxiv.org/pdf/1712.04912.pdf

- Orthogonal Random Forest for Causal Inference
-PDF: http://proceedings.mlr.press/v97/oprescu19a/oprescu19a.pdf

- Policy Learning with Observational Data
-PDF: https://arxiv.org/pdf/1702.02896.pdf

- Adapting Neural Networks for the Estimation of Treatment Effects
-PDF: https://proceedings.neurips.cc/paper/2019/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf

- Decision trees for uplift modeling with single and multiple treatments
-PDF: https://link.springer.com/content/pdf/10.1007/s10115-011-0434-0.pdf

- Estimation and Inference of Heterogeneous Treatment Effects using Random Forests
-PDF: https://arxiv.org/pdf/1510.04342.pdf

- Learning Counterfactual Representations for Estimating Individual Dose-Response Curves
-PDF: https://arxiv.org/abs/1902.00981

- Double/debiased machine learning for treatment and structural parameters 
-PDF: https://academic.oup.com/ectj/article/21/1/C1/5056401?login=true

 
   
 
