# Duxiaoman-learn

数据智能领域优秀文章分享（持续更新中...)

### 1.NLP方向
1. 题目：cosFormer：Rethinking Softmax in Attention
论文PDF: https://arxiv.org/abs/2202.08791

2.  题目：Transformer Quality in Linear Time
 论文PDF: https://arxiv.org/abs/2202.10447

3.  题目：Contextual Representation Learning beyond Masked Language Modeling
论文PDF: http://wiki.duxiaoman-int.com/download/attachments/197406888/2204.04163.pdf?version=1&modificationDate=1652870758000&api=v2

4.  题目：cosFormer：Rethinking Softmax in Attention
论文PDF: https://arxiv.org/abs/2202.08791

5. 题目： Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection
论文PDF: https://arxiv.org/pdf/2203.14380.pdf

6.  题目：Robust Lottery Tickets for Pre-trained Language Models
论文PDF: https://aclanthology.org/2022.acl-long.157/

7.  题目：Compression of Generative Pre-trained Language Models via Quantization
论文PDF: https://arxiv.org/pdf/2203.10705.pdf

8.  题目：Filter-enhanced MLP is All You Need for Sequential Recommendation	论文PDF: https://arxiv.org/abs/2202.13556	

9.  题目：A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models	论文PDF: https://arxiv.org/pdf/2202.13392.pdf

10.  题目：Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents
 论文PDF: https://arxiv.org/abs/2203.02898
 
11.  题目：Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data	
论文PDF: https://arxiv.org/abs/2203.08773

12. 题目： BERT Learns to Teach: Knowledge Distillation with Meta Learning	论文PDF: https://aclanthology.org/2022.acl-long.485/

13.  题目：A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space	论文PDF: https://aclanthology.org/2022.acl-long.336.pdf

14.  题目：LongT5: Efficient Text-To-Text Transformer for Long Sequences	论文PDF: https://arxiv.org/abs/2112.07916

15.  题目：Debiased Contrastive Learning of Unsupervised Sentence Representations	论文PDF: https://arxiv.org/pdf/2205.00656.pdf

16.  题目：DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings	论文PDF: https://aclanthology.org/2022.naacl-main.311.pdf

17.  题目：Poolingformer: Long Document Modeling with Pooling Attention	论文PDF: https://arxiv.org/pdf/2105.04371.pdf

18.  题目：Dict-BERT: Enhancing Language Model Pre-training with Dictionary	论文PDF: https://arxiv.org/pdf/2110.06490.pdf

19.  题目：FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization
论文PDF: https://arxiv.org/abs/2205.07830

20.  题目：Efficient Long-Text Understandingwith Short-Text Models	论文PDF: https://arxiv.org/pdf/2208.00748.pdf

21.  题目：Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification	
论文PDF: https://aclanthology.org/2022.acl-long.158/

22.  题目：ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding	
论文PDF: https://arxiv.org/abs/2109.04380

23.  题目：CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding	
论文PDF: https://openreview.net/forum?id=Ozk9MrX1hvA

24.  题目：An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification	
论文PDF: https://arxiv.org/abs/2210.05529	
