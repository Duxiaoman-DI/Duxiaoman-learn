# Duxiaoman-learn

数据智能领域优秀文章分享（持续更新中...)

   ### 1.NLP方向

 - cosFormer：Rethinking Softmax in Attention 
 -PDF: https://arxiv.org/abs/2202.08791
   
 - Transformer Quality in Linear Time  
 -PDF: https://arxiv.org/abs/2202.10447
   
 - Contextual Representation Learning beyond Masked Language Modeling 
 -PDF:    http://wiki.duxiaoman-int.com/download/attachments/197406888/2204.04163.pdf?version=1&modificationDate=1652870758000&api=v2
 - cosFormer：Rethinking Softmax in Attention 
 -PDF: https://arxiv.org/abs/2202.08791
  
 - Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection 
 -PDF: https://arxiv.org/pdf/2203.14380.pdf

 - Robust Lottery Tickets for Pre-trained Language Models 
 -PDF: https://aclanthology.org/2022.acl-long.157/
     
 - Compression of Generative Pre-trained Language Models via Quantization 
 -PDF: https://arxiv.org/pdf/2203.10705.pdf
      
 - Filter-enhanced MLP is All You Need for Sequential Recommendation	
 -PDF: https://arxiv.org/abs/2202.13556	
     
 - A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models	
 -PDF:    https://arxiv.org/pdf/2202.13392.pdf
      

 - Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents  
 -PDF:https://arxiv.org/abs/2203.02898
 
 -  Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data	 
 -PDF:    https://arxiv.org/abs/2203.08773
   
 -  BERT Learns to Teach: Knowledge Distillation with Meta Learning	
 -PDF: https://aclanthology.org/2022.acl-long.485/
   
 -  A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space	
 -PDF:    https://aclanthology.org/2022.acl-long.336.pdf
   
 -  LongT5: Efficient Text-To-Text Transformer for Long Sequences	
 -PDF: https://arxiv.org/abs/2112.07916
   
 -  Debiased Contrastive Learning of Unsupervised Sentence Representations	
 -PDF: https://arxiv.org/pdf/2205.00656.pdf
   
 -  DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings	
 -PDF: https://aclanthology.org/2022.naacl-main.311.pdf
   
 -  Poolingformer: Long Document Modeling with Pooling Attention	
 -PDF: https://arxiv.org/pdf/2105.04371.pdf
   
 -  Dict-BERT: Enhancing Language Model Pre-training with Dictionary	
 -PDF: https://arxiv.org/pdf/2110.06490.pdf
   
 -  FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization 
 -PDF: https://arxiv.org/abs/2205.07830
   
 -  Efficient Long-Text Understanding with Short-Text Models	
 -PDF: https://arxiv.org/pdf/2208.00748.pdf
   
 -  Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification	 
 -PDF:    https://aclanthology.org/2022.acl-long.158/
   
 -  ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding	 
 -PDF:    https://arxiv.org/abs/2109.04380
   
 -  CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding	 
 -PDF:    https://openreview.net/forum?id=Ozk9MrX1hvA
   
 -  An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification	 
 -PDF:    https://arxiv.org/abs/2210.05529
